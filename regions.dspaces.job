#!/bin/bash  
#SBATCH -J "regions_feng"  
#SBATCH -o "results/%j.out"  
#SBATCH -A prd157
#SBATCH -p shared
#SBATCH -n 13
#SBATCH -N 1
#SBATCH --export=ALL  
#SBATCH -t 00:20:00  

#print the configuration file 
echo "configurations:"
THISFILE=regions.dspaces.job
cat ${THISFILE}
echo "----------------"
echo "----------------"


#This job runs with 3 nodes  
#ibrun in verbose mode will give binding detail  
PBS_O_WORKDIR=/home/rlu/Workspaces/sequentialNpClustering
DS_SERVER=/home/rlu/Dataspacesroot/bin/dataspaces_server
PBS_RESULTDIR=${PBS_O_WORKDIR}/results/${SLURM_JOBID}

mkdir -pv ${PBS_RESULTDIR}
cd $PBS_O_WORKDIR

## Clean up
rm -f conf *.log srv.lck
rm -f dataspaces.conf

## Create dataspaces configuration file
# note that we now have 400 regions
echo "## Config file for DataSpaces
ndim = 3
dims = 500, 500, 500
max_versions = 5
max_readers = 20
lock_type = 2
" > dataspaces.conf

## Run DataSpaces servers
ibrun -n 1 ${DS_SERVER} -s 1 -c 12  >& ${PBS_RESULTDIR}/server.log &
## Give some time for the servers to load and startup
while [ ! -f conf ]; do
    sleep 1s
done
sleep 5s  # wait server to fill up the conf file

## Run region writer and clustering application
ibrun -n 1 bin/put_regions >& ${PBS_RESULTDIR}/producer.log &


## Run region reader application
ibrun -n 10 bin/get_regions >& ${PBS_RESULTDIR}/consumer.log &

## the analysis application
ibrun -n 1 bin/analysis >& ${PBS_RESULTDIR}/analysis.log &

## Wait for the entire workflow to finish
wait
